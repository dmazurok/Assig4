{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh import scoring\n",
    "from whoosh.qparser import QueryParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the TSV file, removing newlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 4154 docs\n"
     ]
    }
   ],
   "source": [
    "def read_file(file_path, delimiter='\\t'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        return [(doc_id, rel, content.replace('\\n', ' ')) for doc_id, rel, content in reader]\n",
    "doc_list = read_file('collection.tsv')\n",
    "print('read {} docs'.format(len(doc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = Schema(id=ID(stored=True), content=TEXT)\n",
    "ix = create_in('cw_index', schema)\n",
    "writer = ix.writer()\n",
    "\n",
    "for doc in doc_list:\n",
    "    writer.add_document(id=doc[0], content=doc[2])\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a helper function for searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_index(ix, query_str, ranking_fn, limit=None):\n",
    "    result_list = []\n",
    "    #with ix.searcher(weighting=ranking_fn) as searcher:\n",
    "    with ix.searcher(weighting=my) as searcher:\n",
    "        query = QueryParser('content', ix.schema).parse(query_str)\n",
    "        results = searcher.search(query, limit=limit)\n",
    "        for result in results:\n",
    "            result_list.append(result['id'])\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the csv file and return a 2-dimensional dictionary:\n",
    "\n",
    "`all_qrels[query][doc_id] = score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_qrels(file_path, delimiter=' '):\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        result = defaultdict(dict)\n",
    "        for query, doc_id, score in reader:\n",
    "            result[query][doc_id] = int(score)\n",
    "    return result\n",
    "all_qrels = read_qrels('q5.web.qrels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the scoring functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def BM25(searcher, fieldname, text, matcher):\n",
    "\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "    \n",
    "    document_length = searcher.doc_field_length(matcher.id(), fieldname, 1)\n",
    "    avg_document_length = searcher.get_parent().avg_field_length(fieldname)\n",
    "    document_frequency = searcher.get_parent().doc_frequency(fieldname, text)\n",
    "    field_length = searcher.get_parent().field_length(fieldname)\n",
    "    try:   \n",
    "        return (math.log10(searcher.get_parent().idf(fieldname, text))*((k1+1)*matcher.value_as('frequency')/(k1*((1-b)+b*document_length/avg_document_length)+matcher.value_as('frequency'))))\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "my = scoring.FunctionWeighting(BM25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now search for all queries using both scoring functions. The query words are separated by underscores in the file, so we have to convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_BM25 = {}\n",
    "results_pos = {}\n",
    "results_BM25_sys = {}\n",
    "for query in all_qrels:\n",
    "    query_new = query.replace('_', ' ')\n",
    "    results_BM25[query] = search_index(ix, query_new, tf_idf_weighting)\n",
    "    results_pos[query] = search_index(ix, query_new, pos_weighting)\n",
    "    results_BM25_sys[query] = search_index(ix, query_new, scoring.BM25F(B=0.75, K1=1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement $\\text{P}@k$.\n",
    "\n",
    "We count the number of documents in our top-$k$ results (`doc_list[:k]`) that have a query relevance larger than $0$ (true positives). We divide by the total number of retrieved items ($k$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(doc_list, qrels, k):\n",
    "    true_pos = len([doc_id for doc_id in doc_list[:k] if qrels.get(doc_id, 0) > 0])\n",
    "    return true_pos / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement recall.\n",
    "\n",
    "We count the number of documents in our results (`doc_list`) that have a query relevance larger than $0$ (true positives). We divide by the total number of relevant items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(doc_list, qrels):\n",
    "    true_pos = len([doc_id for doc_id in doc_list if qrels.get(doc_id, 0) > 0])\n",
    "    total_relevant = len([rel for rel in qrels.values() if rel > 0])\n",
    "    return true_pos / total_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement $\\text{NDCG}@k$.\n",
    "\n",
    "We calculate the DCG using the first $k$ documents by dividing their relevances by the log of the position. Since we have negative relevances in the qrel file, we take the maximum of $0$ and the relevance.\n",
    "\n",
    "The ideal DCG is given by the $k$ highest relevances divided by the logs of their positions. We sort the qrel values and use the top-$k$ ones.\n",
    "\n",
    "We get the normalized DCG by dividing the DCG by the ideal DCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ndcg(doc_list, qrels, k):\n",
    "    dcg = 0\n",
    "    qrels_sorted = sorted(qrels.values(), reverse=True)\n",
    "    idcg = 0\n",
    "    for i in range(1, k + 1):\n",
    "        rel = max(0, qrels.get(doc_list[i - 1], 0))\n",
    "        dcg += rel / math.log2(i + 1)\n",
    "        idcg += max(0, qrels_sorted[i - 1]) / math.log2(i + 1)\n",
    "\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement $\\text{MAP}@k$.\n",
    "\n",
    "For every query, we calculate the average precision, that is, we calculate $P@i$ for all $0 < i \\leq k$ where the $i$'th document was relevant and take the average.\n",
    "\n",
    "The MAP is the mean of all average precision values, i.e. across all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_prec(doc_list, qrels, k):\n",
    "    total = 0\n",
    "    num = 0\n",
    "    for i in range(1, k + 1):\n",
    "        if qrels.get(doc_list[i - 1], 0) > 0:\n",
    "            total += precision(doc_list, qrels, i)\n",
    "            num += 1\n",
    "    if num == 0:\n",
    "        return 0\n",
    "    return total / num\n",
    "\n",
    "def mean_avg_prec(doc_lists, all_qrels, k):\n",
    "    total = 0\n",
    "    for key in doc_lists:\n",
    "        total += avg_prec(doc_lists[key], all_qrels[key], k)\n",
    "    return total / len(doc_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: obama_family_tree\n",
      "85 documents\n",
      "\n",
      "BM25 scoring:\n",
      "P@10 = 0.2\n",
      "R = 0.05813953488372093\n",
      "NDCG@10 = 0.07569867054800501\n",
      "\n",
      "Term position scoring:\n",
      "P@10 = 0.2\n",
      "R = 0.05813953488372093\n",
      "NDCG@10 = 0.07569867054800501\n",
      "\n",
      "Whoosh BM25 scoring:\n",
      "P@10 = 0.2\n",
      "R = 0.05813953488372093\n",
      "NDCG@10 = 0.07569867054800501\n",
      "\n",
      "========================================\n",
      "\n",
      "QUERY: french_lick_resort_and_casino\n",
      "83 documents\n",
      "\n",
      "BM25 scoring:\n",
      "P@10 = 0.8\n",
      "R = 0.273972602739726\n",
      "NDCG@10 = 0.6157082924574933\n",
      "\n",
      "Term position scoring:\n",
      "P@10 = 0.8\n",
      "R = 0.273972602739726\n",
      "NDCG@10 = 0.6157082924574933\n",
      "\n",
      "Whoosh BM25 scoring:\n",
      "P@10 = 0.8\n",
      "R = 0.273972602739726\n",
      "NDCG@10 = 0.6157082924574933\n",
      "\n",
      "========================================\n",
      "\n",
      "QUERY: getting_organized\n",
      "468 documents\n",
      "\n",
      "BM25 scoring:\n",
      "P@10 = 0.8\n",
      "R = 0.5641025641025641\n",
      "NDCG@10 = 0.8364586133797037\n",
      "\n",
      "Term position scoring:\n",
      "P@10 = 0.8\n",
      "R = 0.5641025641025641\n",
      "NDCG@10 = 0.8364586133797037\n",
      "\n",
      "Whoosh BM25 scoring:\n",
      "P@10 = 0.8\n",
      "R = 0.5641025641025641\n",
      "NDCG@10 = 0.8364586133797037\n",
      "\n",
      "========================================\n",
      "\n",
      "QUERY: toilet\n",
      "535 documents\n",
      "\n",
      "BM25 scoring:\n",
      "P@10 = 0.0\n",
      "R = 0.4444444444444444\n",
      "NDCG@10 = 0.0\n",
      "\n",
      "Term position scoring:\n",
      "P@10 = 0.0\n",
      "R = 0.4444444444444444\n",
      "NDCG@10 = 0.0\n",
      "\n",
      "Whoosh BM25 scoring:\n",
      "P@10 = 0.0\n",
      "R = 0.4444444444444444\n",
      "NDCG@10 = 0.0\n",
      "\n",
      "========================================\n",
      "\n",
      "QUERY: mitchell_college\n",
      "318 documents\n",
      "\n",
      "BM25 scoring:\n",
      "P@10 = 0.1\n",
      "R = 0.25\n",
      "NDCG@10 = 0.053905577064733715\n",
      "\n",
      "Term position scoring:\n",
      "P@10 = 0.1\n",
      "R = 0.25\n",
      "NDCG@10 = 0.053905577064733715\n",
      "\n",
      "Whoosh BM25 scoring:\n",
      "P@10 = 0.1\n",
      "R = 0.25\n",
      "NDCG@10 = 0.053905577064733715\n",
      "\n",
      "========================================\n",
      "\n",
      "BM25 scoring:\n",
      "MAP@10 = 0.41452380952380946\n",
      "Term position scoring:\n",
      "MAP@10 = 0.41452380952380946\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "for query in all_qrels:\n",
    "    print('QUERY: {}\\n'\n",
    "          '{} documents\\n'.format(query, len(results_pos[query])))\n",
    "    \n",
    "    p_at_k = precision(results_tf_idf[query], all_qrels[query], k)\n",
    "    r = recall(results_BM25[query], all_qrels[query])\n",
    "    ndcg_at_k = ndcg(results_tf_idf[query], all_qrels[query], k)\n",
    "    print('BM25F scoring:\\n'\n",
    "          'P@{} = {}\\n'\n",
    "          'R = {}\\n'\n",
    "          'NDCG@{} = {}\\n'.format(k, p_at_k, r, k, ndcg_at_k))\n",
    "    \n",
    "    p_at_k = precision(results_pos[query], all_qrels[query], k)\n",
    "    r = recall(results_pos[query], all_qrels[query])\n",
    "    ndcg_at_k = ndcg(results_pos[query], all_qrels[query], k)\n",
    "    print('Term position scoring:\\n'\n",
    "          'P@{} = {}\\n'\n",
    "          'R = {}\\n'\n",
    "          'NDCG@{} = {}\\n'.format(k, p_at_k, r, k, ndcg_at_k))\n",
    "    \n",
    "    p_at_k = precision(results_BM25_sys[query], all_qrels[query], k)\n",
    "    r = recall(results_BM25_sys[query], all_qrels[query])\n",
    "    ndcg_at_k = ndcg(results_tfidf_sys[query], all_qrels[query], k)\n",
    "    print('Whoosh BM25F scoring:\\n'\n",
    "          'P@{} = {}\\n'\n",
    "          'R = {}\\n'\n",
    "          'NDCG@{} = {}'.format(k, p_at_k, r, k, ndcg_at_k))\n",
    "\n",
    "    print('\\n========================================\\n')\n",
    "\n",
    "map_at_k = mean_avg_prec(results_BM25, all_qrels, k)\n",
    "print('BM25F scoring:\\n'\n",
    "      'MAP@{} = {}'.format(k, map_at_k))\n",
    "map_at_k = mean_avg_prec(results_pos, all_qrels, k)\n",
    "print('Term position scoring:\\n'\n",
    "      'MAP@{} = {}'.format(k, map_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
